{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2789b86",
   "metadata": {},
   "source": [
    "# Agentic Patterns Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4cb8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from typing import Literal, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e97c693",
   "metadata": {},
   "source": [
    "## Adaptive Reflection with Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"paid_api\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API_Key not found. Please set it in your .env file\")\n",
    "print(\"API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad242ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    api_key = api_key\n",
    ")\n",
    "print(f\"LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b7cbc",
   "metadata": {},
   "source": [
    "### Reflection Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a508daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityScore(BaseModel):\n",
    "    \"\"\"Quality scoring model.\"\"\"\n",
    "    clarity: int = Field(ge=1, le=5)\n",
    "    completeness: int = Field(ge=1, le=5)\n",
    "    accuracy: int = Field(ge=1, le=5)\n",
    "\n",
    "    def is_high_quality(self) -> bool:\n",
    "        \"\"\"Check if all scores are >= 4.\"\"\"\n",
    "        return all(score >= 4 for score in [self.clarity, self.completeness, self.accuracy])\n",
    "\n",
    "class MetricReflectionState(TypedDict):\n",
    "    \"\"\"State for metric-based reflection.\"\"\"\n",
    "    task: str\n",
    "    draft: str\n",
    "    iterations: int\n",
    "    scores: list[QualityScore]\n",
    "    critique: str\n",
    "    iterations: int\n",
    "    final_output: str\n",
    "\n",
    "MAX_METRIC_ITERATIONS = 3\n",
    "QUALITY_THRESHOLD = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_generator(state: MetricReflectionState) -> dict:\n",
    "    \"\"\"Generate or refine based on critique.\"\"\"\n",
    "    if state[\"iterations\"] == 0:\n",
    "        prompt = f\"Create a response for: {state['task']}\"\n",
    "        print(\"\\nGenerating initial draft...\")\n",
    "    else:\n",
    "        prompt = f\"\"\"Improve this draft:\n",
    "\n",
    "Task: {state['task']}\n",
    "Draft: {state['draft']}\n",
    "Critique: {state['critique']}\n",
    "\n",
    "Create improved version.\"\"\"\n",
    "        print(f\"\\nRefining (iteration {state['iterations']})...\")\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    print(\"‚úì Draft created\\n\")\n",
    "    return {\n",
    "        \"draft\": response.content\n",
    "    } #Include iteration\n",
    "\n",
    "\n",
    "def metric_critic(state: MetricReflectionState) -> dict:\n",
    "    \"\"\"Score draft on quality metrics.\"\"\"\n",
    "    llm_with_structure = llm.with_structured_output(QualityScore)\n",
    "    \n",
    "    prompt = f\"\"\"Evaluate this response:\n",
    "\n",
    "Task: {state['task']}\n",
    "Response: {state['draft']}\n",
    "\n",
    "Score on:\n",
    "- Clarity (1-5): How clear and understandable?\n",
    "- Completeness (1-5): How complete is the answer?\n",
    "- Accuracy (1-5): How accurate is the information?\n",
    "\n",
    "Provide scores and reasoning.\"\"\"\n",
    "    \n",
    "    print(\"üîç Scoring draft...\")\n",
    "    score = llm_with_structure.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    print(f\"Scores - Clarity: {score.clarity}, Completeness: {score.completeness}, Accuracy: {score.accuracy}\")\n",
    "    print(f\"Reasoning: {score.reasoning[:80]}...\\n\")\n",
    "    \n",
    "    # Build critique from low scores\n",
    "    low_scores = []\n",
    "    if score.clarity < QUALITY_THRESHOLD:\n",
    "        low_scores.append(f\"Clarity needs improvement (score: {score.clarity})\")\n",
    "    if score.completeness < QUALITY_THRESHOLD:\n",
    "        low_scores.append(f\"Completeness needs work (score: {score.completeness})\")\n",
    "    if score.accuracy < QUALITY_THRESHOLD:\n",
    "        low_scores.append(f\"Accuracy could be better (score: {score.accuracy})\")\n",
    "    \n",
    "    critique = \"; \".join(low_scores) if low_scores else \"APPROVED\"\n",
    "    \n",
    "    return {\n",
    "        \"scores\": [score],\n",
    "        \"critique\": critique,\n",
    "        \"iterations\": state[\"iterations\"] + 1\n",
    "    }\n",
    "\n",
    "def metric_finalizer(state: MetricReflectionState) -> dict:\n",
    "    \"\"\"Finalize with score summary.\"\"\"\n",
    "    print(\"\\nReflection complete!\\n\")\n",
    "    \n",
    "    return {\"final_output\": state[\"draft\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_metric_reflect(state: MetricReflectionState) -> Literal[\"generator\", \"finalizer\"]:\n",
    "    \"\"\"Check if all scores meet threshold.\"\"\"\n",
    "    if not state.get(\"scores\"):\n",
    "        return \"generator\"\n",
    "    \n",
    "    latest_score = state[\"scores\"][-1]\n",
    "    all_good = (latest_score.clarity >= QUALITY_THRESHOLD and \n",
    "                latest_score.completeness >= QUALITY_THRESHOLD and \n",
    "                latest_score.accuracy >= QUALITY_THRESHOLD)\n",
    "    \n",
    "    if all_good or state[\"iterations\"] >= MAX_METRIC_ITERATIONS:\n",
    "        return \"finalizer\"\n",
    "    \n",
    "    return \"generator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d852fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_builder = StateGraph(MetricReflectionState)\n",
    "reflection_builder.add_node(\"generator\", metric_generator)\n",
    "reflection_builder.add_node(\"critic\", metric_critic)\n",
    "reflection_builder.add_node(\"finalizer\", metric_finalizer)\n",
    "\n",
    "reflection_builder.add_edge(START, \"generator\")\n",
    "reflection_builder.add_edge(\"generator\", \"critic\")\n",
    "reflection_builder.add_conditional_edges(\n",
    "    \"critic\",\n",
    "    should_metric_reflect,\n",
    "    {\"generator\": \"generator\", \"finalizer\": \"finalizer\"}\n",
    ")\n",
    "reflection_builder.add_edge(\"finalizer\", END)\n",
    "\n",
    "reflection_agent = reflection_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab51fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    display(Image(reflection_agent.get_graph().draw_mermaid_png()))\n",
    "except:\n",
    "    print(\"Graph visualization requires mermaid support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a07d4b",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dbcb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1 = \"Explain distributed power supply in simple terms\"\n",
    "\n",
    "result = reflection_agent.invoke({\n",
    "    \"task\": task1,\n",
    "    \"draft\": \"\",\n",
    "    \"iterations\": 0,\n",
    "    \"scores\": [],\n",
    "    \"critique\": \"\",\n",
    "    \"final_output\": \"\"\n",
    "    \n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üìä FINAL OUTPUT (after reflection):\")\n",
    "print(f\"{'='*70}\")\n",
    "print(result[\"final_output\"])\n",
    "print(f\"\\nTotal iterations: {result['iterations']}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2b1afa",
   "metadata": {},
   "source": [
    "## Plan-Execute + Reflection Hybrid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a_Deep (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
