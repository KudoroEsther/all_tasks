{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "057bd4cf",
   "metadata": {},
   "source": [
    "# RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acfb668",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd7720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries...\n",
      "libraries successfully imported!\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing libraries...\")\n",
    "#loading and chunking libraries\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# vector storage\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "#LECL libraries\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Conversational memory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "print(\"libraries successfully imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c01bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"paid_api\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "print(\"API key loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3534112",
   "metadata": {},
   "source": [
    "## Loading and chunking documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f7ae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded.\n",
      "document loaded and chunked!\n"
     ]
    }
   ],
   "source": [
    "# data_path = r\"C:\\Users\\owner\\Desktop\\Files_Deep_Learning\\RAG\\Project\\documents\"\n",
    "\n",
    "data_path = r\"C:\\Users\\owner\\Desktop\\Files_Deep_Learning\\RAG\\Project\\documents\"\n",
    "# Loading document\n",
    "documents = []\n",
    "for file in os.listdir(data_path):\n",
    "    file_path = os.path.join(data_path, file)\n",
    "\n",
    "    if file.endswith(\".txt\"):\n",
    "        loader = TextLoader(file_path, encoding='utf-8')\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "    elif file.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "print(\"Documents loaded.\")\n",
    "\n",
    "#Chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap = 50)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(\"document loaded and chunked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc56d9e",
   "metadata": {},
   "source": [
    "## Storing embeddings in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80375bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created and saved to chroma_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22832\\3192814298.py:6: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Creating embeddings and storing\n",
    "embedding = OpenAIEmbeddings(model= \"text-embedding-3-small\", openai_api_key=api_key)\n",
    "vectorstore = Chroma.from_documents(documents=chunks,\n",
    "                                    embedding=embedding, persist_directory = \"./chroma_db\")\n",
    "\n",
    "vectorstore.persist()\n",
    "print(f\"Embeddings created and saved to chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c563b90",
   "metadata": {},
   "source": [
    "## RAG Chain with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a9dac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0,openai_api_key=api_key)\n",
    "\n",
    "# Retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "system_prompt_text = \"\"\"\n",
    "You are a personal RAG assistant answering questions strictly from the provided context about Esther Kudoro.\n",
    "\n",
    "### INSTRUCTIONS:\n",
    "1. Answer questions about professional experience, skills, repositories, and technical implementation details.\n",
    "2. Use ONLY the context below. If the answer is not present, say \"I do not have that information.\"\n",
    "3. ALWAYS cite your sources implicitly by referring to the specific file or section.\n",
    "4. Format all responses as clean plain text with no markdown or special characters.\n",
    "\n",
    "### PRIVACY GUARDRAILS (CRITICAL):\n",
    "You MUST REFUSE to answer questions about the following personal sensitive information, even if it might be present in the context:\n",
    "- Age\n",
    "- Date of birth\n",
    "- Home Address\n",
    "- Phone number\n",
    "- Personal Email address\n",
    "- Any other sensitive personal identifiers\n",
    "\n",
    "If a user asks for this information, reply EXACTLY with:\n",
    "\"I cannot share personal or sensitive information such as contact details or age. Please ask about her professional experience or projects.\"\n",
    "\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer clearly and concisely.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt_text),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c669e",
   "metadata": {},
   "source": [
    "## Initialising and activating conversational memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f0e5b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chain created!\n"
     ]
    }
   ],
   "source": [
    "# RAG CHAIN\n",
    "from operator import itemgetter\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\")\n",
    "        | retriever\n",
    "        | format_docs,\n",
    "\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG Chain created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b82ba",
   "metadata": {},
   "source": [
    "## Running the RAG assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289ec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esther Kudoro Personal Assistant \n",
      "Type either 'exit' or 'quit' to stop the program.\n",
      "\n",
      "User: who is esther kudoro\n",
      "Assistant: Esther Kudoro is an Electrical and Electronics Engineer with first-class honors. She has a strong interest in power systems engineering, data analytics, and machine learning. Esther focuses on applying artificial intelligence and data-driven techniques to engineering problems, with professional interests in fault diagnosis, predictive maintenance, and retrieval-augmented generation systems. She possesses technical skills in electrical and electronics engineering, power systems engineering, electrical fault analysis, and has experience with Python for data science and machine learning tasks.\n",
      "\n",
      "\n",
      "User: what are esther kudoro\n",
      "Assistant: I do not have that information.\n",
      "\n",
      "\n",
      "User: what are esther kudoro's core skills\n",
      "Assistant: Esther Kudoro's core skills include electrical and electronics engineering, power systems engineering, electrical fault analysis, data analysis, feature engineering, and classification models. She also has experience working with Python for data science and machine learning tasks.\n",
      "\n",
      "\n",
      "User: what is her home address\n",
      "Assistant: I cannot share personal or sensitive information such as contact details or age. Please ask about her professional experience or projects.\n",
      "\n",
      "\n",
      "User: did esther kudoro attend Afe Babalola University\n",
      "Assistant: Yes, Esther Kudoro attended Afe Babalola University, where she studied Electrical and Electronics Engineering and graduated with a first-class degree.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "print(\"Esther Kudoro Personal Assistant \\nType either 'exit' or 'quit' to stop the program.\")\n",
    "\n",
    "while True:\n",
    "    question = input(\"You: \")\n",
    "    if question.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    answer = rag_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "\n",
    "    print(f\"\\nUser: {question}\")\n",
    "    print(f\"Assistant: {answer}\\n\")\n",
    "\n",
    "    chat_history.extend([\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=answer)\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a_deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
