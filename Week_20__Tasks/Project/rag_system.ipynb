{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "057bd4cf",
   "metadata": {},
   "source": [
    "# RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acfb668",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56cd7720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\owner\\Desktop\\Files_Deep_Learning\\a_deep\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries successfully imported!\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing libraries...\")\n",
    "#loading and chunking libraries\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# vector storage\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "#LECL librarie\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Conversational memory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "print(\"libraries successfully imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c01bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"paid_api\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "print(\"API key loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3534112",
   "metadata": {},
   "source": [
    "## Loading and chunking documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f7ae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document loaded and chunked!\n"
     ]
    }
   ],
   "source": [
    "data_path = r\"C:\\Users\\owner\\Desktop\\Files_Deep_Learning\\RAG\\Project\\documents\"\n",
    "\n",
    "# Loading document\n",
    "documents = []\n",
    "for file in os.listdir(data_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        loader = TextLoader(os.path.join(data_path, file), encoding='utf-8')\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "#Chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap = 50)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(\"document loaded and chunked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc56d9e",
   "metadata": {},
   "source": [
    "## Storing embeddings in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80375bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created and saved to chroma_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_8564\\3192814298.py:6: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Creating embeddings and storing\n",
    "embedding = OpenAIEmbeddings(model= \"text-embedding-3-small\", openai_api_key=api_key)\n",
    "vectorstore = Chroma.from_documents(documents=chunks,\n",
    "                                    embedding=embedding, persist_directory = \"./chroma_db\")\n",
    "\n",
    "vectorstore.persist()\n",
    "print(f\"Embeddings created and saved to chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c563b90",
   "metadata": {},
   "source": [
    "## RAG Chain with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a9dac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0,openai_api_key=api_key)\n",
    "\n",
    "# Retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "system_prompt_text = \"\"\"\n",
    "You are a personal RAG assistant answering questions strictly from the provided context about Esther Kudoro.\n",
    "\n",
    "### INSTRUCTIONS:\n",
    "1. Answer questions about professional experience, skills, repositories, and technical implementation details.\n",
    "2. Use ONLY the context below. If the answer is not present, say \"I do not have that information.\"\n",
    "3. ALWAYS cite your sources implicitly by referring to the specific file or section.\n",
    "4. Format all responses as clean plain text with no markdown or special characters.\n",
    "\n",
    "### PRIVACY GUARDRAILS (CRITICAL):\n",
    "You MUST REFUSE to answer questions about the following personal sensitive information, even if it might be present in the context:\n",
    "- Age\n",
    "- Date of birth\n",
    "- Home Address\n",
    "- Phone number\n",
    "- Personal Email address\n",
    "- Any other sensitive personal identifiers\n",
    "\n",
    "If a user asks for this information, reply EXACTLY with:\n",
    "\"I cannot share personal or sensitive information such as contact details or age. Please ask about her professional experience or projects.\"\n",
    "\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer clearly and concisely.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt_text),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c669e",
   "metadata": {},
   "source": [
    "## Initialising and activating conversational memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f0e5b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chain created!\n"
     ]
    }
   ],
   "source": [
    "# RAG CHAIN\n",
    "from operator import itemgetter\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\")\n",
    "        | retriever\n",
    "        | format_docs,\n",
    "\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG Chain created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b82ba",
   "metadata": {},
   "source": [
    "## Running the RAG assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e289ec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esther Kudoro Personal Assistant \n",
      "Type either 'exit' or 'quit' to stop the program.\n",
      "Assistant: Esther Kudoro is an Electrical and Electronics Engineer with first-class honors. She has a strong interest in power systems engineering, data analytics, and machine learning. Esther focuses on applying artificial intelligence and data-driven techniques to engineering problems, with professional interests in fault diagnosis, predictive maintenance, and retrieval-augmented generation systems. She possesses technical skills in electrical and electronics engineering, power systems engineering, electrical fault analysis, and has experience with Python for data science and machine learning tasks.\n",
      "\n",
      "Assistant: I do not have specific information about Esther Kudoro's professional experiences or work history. However, she has a strong background in electrical and electronics engineering, power systems engineering, data analytics, and machine learning, with a focus on applying AI and data-driven techniques to engineering problems.\n",
      "\n",
      "Assistant: Esther Kudoro's GitHub account contains multiple public repositories used for learning, practice tasks, fellowship exams, and datasets. The repositories include AI_Fellowship_Exam, all_tasks, Dataset, and other code repositories that showcase her use of Python and Jupyter Notebook. These projects reflect her hands-on practice in Python programming, data analysis, and machine learning exercises.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "print(\"Esther Kudoro Personal Assistant \\nType either 'exit' or 'quit' to stop the program.\")\n",
    "\n",
    "while True:\n",
    "    question = input(\"You: \")\n",
    "    if question.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    answer = rag_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "\n",
    "    print(f\"Assistant: {answer}\\n\")\n",
    "\n",
    "    chat_history.extend([\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=answer)\n",
    "    ])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a_deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
